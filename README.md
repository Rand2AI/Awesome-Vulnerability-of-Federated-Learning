# Vulnerabilities and Defenses of Federated Learning
This is a list of papers reviewed in [A Survey on Vulnerability of Federated Learning: A Learning Algorithm Perspective](https://www.sciencedirect.com/science/article/pii/S0925231223013486).

In this paper, we propose a taxonomy of FL attacks centered around attack origins and attack targets, shown in the table below. Our taxonomy of FL attacks emphasizes exploited vulnerabilities and their direct victims.

<center>
| Type of Attack    | Definition | Example|
| -------- | ------- | ------- |
| Data to Model (D2M) | Tampering the data alone to degrade model performance    | Label-flipping |
| Model to Model (M2M)| Tampering updates to prevent learning convergence     | Byzantine attack|
| Model to Data (M2D) | Intercepting model updates to inference private data information   | Gradient leakage|
| Composite (D2M+M2M) | Tampering both data and updates to manipulate model behavior | Backdoor injection |
<\center>

![overview](overview.png)

## D2M
We describe Data to Model (D2M) attacks in FL as threat models that are launched by manipulating the local data while the models in training are being targeted as victims. D2M attacks are also considered as black-box attacks because the attackers do not need to access inside information such as client model weights or updates, tampering the data alone is often suffice to launch a D2M attack. However, the attackers can also draw information from local dataset or client models to enhance the effectiveness of D2M attacks.

### Attacks
Poisoning Attacks against Support Vector Machines [ACM ICML](https://dl.acm.org/doi/10.5555/3042573.3042761)

Mitigating Sybils in Federated Learning Poisoning [arXiv](https://arxiv.org/abs/1808.04866)

Data Poisoning Attacks Against Federated Learning Systems [SPRINGER ESORICS](https://dl.acm.org/doi/10.1007/978-3-030-58951-6_24)

Semi-Targeted Model Poisoning Attack on Federated Learning via Backward Error Analysis [arXiv](https://arxiv.org/abs/2203.11633)

Attack of the Tails: Yes, You Really Can Backdoor Federated Learning [NeurIPS](https://proceedings.neurips.cc/paper/2020/hash/b8ffa41d4e492f0fad2f13e29e1762eb-Abstract.html)

PoisonGAN: Generative Poisoning Attacks Against Federated Learning in Edge Computing Systems [IEEE ITJ](https://ieeexplore.ieee.org/document/9194010)

Turning Federated Learning Systems Into Covert Channels [IEEE Access](https://ieeexplore.ieee.org/document/9984638)

Challenges and Approaches for Mitigating Byzantine Attacks in Federated Learning [arXiv](https://arxiv.org/abs/2112.14468)

Turning Privacy-preserving Mechanisms against Federated Learning [arXiv](https://arxiv.org/abs/2305.05355)

Local Environment Poisoning Attacks on Federated Reinforcement Learning [arXiv](https://arxiv.org/abs/2303.02725)

Data Poisoning Attacks on Federated Machine Learning [arXiv](https://arxiv.org/abs/2004.10020)

Understanding Distributed Poisoning Attack in Federated Learning [IEEE ICPADS](https://ieeexplore.ieee.org/document/8975792)

### Defenses
Mitigating Sybils in Federated Learning Poisoning [arXiv](https://arxiv.org/abs/1808.04866)

Data Poisoning Attacks Against Federated Learning Systems [SPRINGER ESORICS](https://dl.acm.org/doi/10.1007/978-3-030-58951-6_24)

Understanding Distributed Poisoning Attack in Federated Learning [IEEE ICPADS](https://ieeexplore.ieee.org/document/8975792)

Local Environment Poisoning Attacks on Federated Reinforcement Learning [arXiv](https://arxiv.org/abs/2303.02725)


## M2M
We define Model to Model (M2M) attacks in FL as threat models that manipulate local model updates or weights to affect the global model. The primary objective of an M2M attack is to disrupt the convergence of FL algorithms. The presence of M2M attacks is also described as the Byzantine problem. In a distributed system affected by the Byzantine problem, benign and malicious participants coexist in the system. Malicious participants deliberately disseminate confusing or contradicting information to undermine the systemâ€™s normal operations. Therefore the challenge for the system administrator lies in achieving consensus among benign participants despite the presence of malicious ones.

### Attacks
Free-rider Attacks on Model Aggregation in Federated Learning [PMLR](http://proceedings.mlr.press/v130/fraboni21a)

Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent [NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html)

Generalized Byzantine-tolerant SGD [arXiv](https://arxiv.org/abs/1802.10116)

RSA: Byzantine-robust stochastic aggregation methods for distributed learning from heterogeneous datasets [AAAI](https://dl.acm.org/doi/10.1609/aaai.v33i01.33011544)

A Little Is Enough: Circumventing Defenses For Distributed Learning [NeurIPS](https://proceedings.neurips.cc/paper/2019/hash/ec1c59141046cd1866bbbcdfb6ae31d4-Abstract.html)

The Hidden Vulnerability of Distributed Learning in Byzantium [PMLR](http://proceedings.mlr.press/v80/mhamdi18a)

Local model poisoning attacks to byzantine-robust federated learning [ACM SEC](https://dl.acm.org/doi/abs/10.5555/3489212.3489304)

PipAttack: Poisoning Federated Recommender Systems for Manipulating Item Promotion [ACM WSDM](https://dl.acm.org/doi/10.1145/3488560.3498386)

FedRecAttack: Model Poisoning Attack to Federated Recommendation [IEEE ICDE](https://ieeexplore.ieee.org/document/9835228)

Poisoning Deep Learning Based Recommender Model in Federated Learning Scenarios [IJCAI](https://arxiv.org/abs/2204.13594)

### Defenses
Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent [NeuIPS](https://papers.nips.cc/paper_files/paper/2017/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html)

Generalized Byzantine-tolerant SGD [arXiv](https://arxiv.org/abs/1802.10116)

Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates [ICML](https://proceedings.mlr.press/v80/yin18a.html)

Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent [ACM](https://dl.acm.org/doi/10.1145/3154503)

Robust Aggregation for Federated Learning [IEEE](https://ieeexplore.ieee.org/document/9721118)

ELITE: Defending Federated Learning against Byzantine Attacks based on Information Entropy [IEEE](https://ieeexplore.ieee.org/document/9727486)

## M2D
We summarize the Model to Data (M2D) attacks in FL to be non-gradient-based leakage and gradient-based data leakage.

We define non-gradient-based data leakage as the disclosure of private information that occurs independently of the gradient generated during the training stage. For instance, the leakage can involve identifying specific attributes or membership details within the training data, or recovering original training images from obscured or masked versions. Typically, such leakage exploits the capabilities of a well-trained model to execute these attacks.

Gradient-based data leakage refers to techniques that exploit gradients from the target model to expose privacy-sensitive information. Deep learning models are trained on datasets, and parameter updates occur through alignment with the feature space. This establishes an inherent relationship between the weights or gradients and the dataset. Consequently, numerous studies aim to reveal private information by leveraging these gradients. The effectiveness and success rates of gradient-based approaches have consistently surpassed those of non-gradient-based methods. Unlike non-gradient-based leakage, gradient-based data leakage can occur even in models that have not yet converged.

### Attacks
Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers [arXiv](https://arxiv.org/pdf/1306.4447.pdf)

Membership inference attacks against machine learning models [IEEE SP](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7958568)

Defeating image obfuscation with deep learning [arXiv](https://arxiv.org/pdf/1609.00408.pdf)

The secret revealer: Generative model-inversion attacks against deep neural networks [IEEE CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_The_Secret_Revealer_Generative_Model-Inversion_Attacks_Against_Deep_Neural_Networks_CVPR_2020_paper.pdf)

Deep Models under the GAN: Information Leakage from Collaborative Deep Learning [ACM CCCS](https://dl.acm.org/doi/pdf/10.1145/3133956.3134012)

Exploiting Unintended Feature Leakage in Collaborative Learning [IEEE SP](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8835269)

Auditing Privacy Defenses in Federated Learning via Generative Gradient Leakage [IEEE CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Auditing_Privacy_Defenses_in_Federated_Learning_via_Generative_Gradient_Leakage_CVPR_2022_paper.pdf)

Deep Leakage from Gradients [NeurIPS](https://proceedings.neurips.cc/paper/2019/file/60a6c4002cc7b29142def8871531281a-Paper.pdf)

Idlg: Improved Deep Leakage from Gradients [arXiv](https://arxiv.org/pdf/2001.02610.pdf)

Inverting Gradients-How Easy Is It to Break Privacy in Federated Learning? [NeurIPS](https://proceedings.neurips.cc/paper/2020/file/c4ede56bbd98819ae6112b20ac6bf145-Paper.pdf)

GRNN: Generative Regression Neural Network: A Data Leakage Attack for Federated Learning [ACM TIST](https://dl.acm.org/doi/abs/10.1145/3510032)

Gradient Inversion with Generative Image Prior [NeurIPS](https://proceedings.neurips.cc/paper/2021/file/fa84632d742f2729dc32ce8cb5d49733-Paper.pdf)

See through Gradients: {{Image}} Batch Recovery via Gradinversion [IEEE CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Yin_See_Through_Gradients_Image_Batch_Recovery_via_GradInversion_CVPR_2021_paper.pdf)

Beyond Inferring Class Representatives: {{User-level}} Privacy Leakage from Federated Learning [IEEE ICCC](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8737416)

### Defenses

An Accuracy-Lossless Perturbation Method for Defending Privacy Attacks in Federated Learning [ACM WC](https://dl.acm.org/doi/pdf/10.1145/3485447.3512233)

LDP-FL: Practical private aggregation in federated learning with local differential privacy [arXiv](https://arxiv.org/pdf/2007.15789.pdf)

Soteria: Provable Defense against Privacy Leakage in Federated Learning from Representation Perspective [IEEE CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Sun_Soteria_Provable_Defense_Against_Privacy_Leakage_in_Federated_Learning_From_CVPR_2021_paper.pdf)

An effective value swapping method for privacy preserving data publishing [SCN](https://onlinelibrary.wiley.com/doi/pdf/10.1002/sec.1527)

Efficient data perturbation for privacy preserving and accurate data stream mining [ELSEVIER PMC](https://www.sciencedirect.com/science/article/pii/S1574119217305229)

Efficient privacy preservation of big data for accurate data mining [ELSEVIER IS](https://www.sciencedirect.com/science/article/pii/S0020025519304578)

Digestive neural networks: A novel defense strategy against inference attacks in federated learning [ELSEVIER CS](https://www.sciencedirect.com/science/article/pii/S0167404821002029)

Privacy preserving distributed machine learning with federated learning [ELSEVIER CC](https://www.sciencedirect.com/science/article/pii/S0140366421000773)

Deep learning with gaussian differential privacy [HDSR](https://assets.pubpub.org/37cmylj9/51597243686372.pdf)

FedBoosting: Federated Learning with Gradient Protected Boosting for Text Recognition [ELSEVIER NEUROCOMPUTING](https://www.sciencedirect.com/science/article/pii/S0925231223012493)

Privacy-preserving federated learning framework based on chained secure multiparty computing [IEEE ITJ](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9187932)

Differential privacy approach to solve gradient leakage attack in a federated machine learning environment [SPRINGER ICCDSN](https://link.springer.com/chapter/10.1007/978-3-030-66046-8_31)

Gradient-leakage resilient federated learning [IEEE ICDCS](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9546481)

Gradient Leakage Defense with Key-Lock Module for Federated Learning [arXiv](https://arxiv.org/pdf/2305.04095.pdf)

PRECODE-A Generic Model Extension to Prevent Deep Gradient Leakage [IEEE CVPR](https://openaccess.thecvf.com/content/WACV2022/papers/Scheliga_PRECODE_-_A_Generic_Model_Extension_To_Prevent_Deep_Gradient_WACV_2022_paper.pdf)

## Composite
We define composite attacks as threat models that corrupt multiple aspects of FL. The attacker can combine D2M and M2M attacks to launch backdoor attacks. The attacker surreptitiously adds trigger patterns to local training data, then poisons model updates such that the global model learns how to react to triggers. Backdoored models behave normally when fed with clean data. In the presence of trigger data, these models are trained to give predictions designated by the attacker. Compared to D2M or M2M attacks, now that the attacker also has control over client model updates, composite attacks tend to be stealthier and more destructive.

### Attacks
Analyzing Federated Learning through an Adversarial Lens [ICML](https://proceedings.mlr.press/v97/bhagoji19a.html)

How To Backdoor Federated Learning [ICAIS](https://proceedings.mlr.press/v108/bagdasaryan20a.html)

Can You Really Backdoor Federated Federated Learning? [NeurIPS Workshop](https://arxiv.org/abs/1911.07963)

Attack of the Tails: Yes, You Really Can Backdoor Federated Learning [NeurIPS](https://proceedings.neurips.cc/paper/2020/hash/b8ffa41d4e492f0fad2f13e29e1762eb-Abstract.html)

A Little Is Enough: Circumventing Defenses For Distributed Learning [NeurIPS](https://proceedings.neurips.cc/paper/2019/hash/ec1c59141046cd1866bbbcdfb6ae31d4-Abstract.html)

DBA: Distributed Backdoor Attacks against Federated Learning [ICLR](https://openreview.net/forum?id=rkgyS0VFvr)

Coordinated Backdoor Attacks against Federated Learning with Model-Dependent Triggers [IEEE Networks](https://ieeexplore.ieee.org/document/9713908)

Neurotoxin: Durable Backdoors in Federated Learning [ICML](https://proceedings.mlr.press/v162/zhang22w.html)

Learning to backdoor federated learning [ICLR Workshop](https://arxiv.org/pdf/2303.03320.pdf)

On the Vulnerability of Backdoor Defenses for Federated Learning [NeurIPS Workshop](https://openreview.net/forum?id=awOO1NGKZIx)

Backdoor Attacks in Federated Learning by Rare Embeddings and Gradient Ensembling [EMNLP](https://aclanthology.org/2022.emnlp-main.6/)

Thinking two moves ahead: Anticipating other users improves backdoor attacks in federated learning [arXiv](https://arxiv.org/abs/2210.09305)

Accumulative Poisoning Attacks on Real-time Data [NeurIPS](https://openreview.net/forum?id=4CrjylrL9vM)

### Defenses
Defending against Backdoors in Federated Learning with Robust Learning Rate [AAAI](https://ojs.aaai.org/index.php/AAAI/article/view/17118)

Learning Differentially Private Recurrent Language Models [ICLR](https://openreview.net/forum?id=BJ0hF1Z0b)

Mitigating Backdoor Attacks in Federated Learning [arXiv](https://arxiv.org/abs/2011.01767)

FedRAD: Federated Robust Adaptive Distillation [NeuIPS Workshop](https://arxiv.org/abs/2112.01405)

CRFL: Certifiably Robust Federated Learning against Backdoor Attacks [ICML](https://arxiv.org/abs/2106.08283)

FLCert: Provably Secure Federated Learning Against Poisoning Attacks [IEEE](https://ieeexplore.ieee.org/document/9911773)

BaFFLe: Backdoor Detection via Feedback-based Federated Learning [IEEE](https://www.computer.org/csdl/proceedings-article/icdcs/2021/451300a852/1xqyZHylA8E)

DeepSight: Mitigating Backdoor Attacks in Federated Learning Through Deep Model Inspection [NDSS](https://arxiv.org/abs/2201.00763)

FLAME: Taming Backdoors in Federated Learning [USENIX](https://www.usenix.org/conference/usenixsecurity22/presentation/nguyen)


# Citation

```
  @article{XIE2024127225,
      title = {A survey on vulnerability of federated learning: A learning algorithm perspective},
      author = {Xianghua Xie and Chen Hu and Hanchi Ren and Jingjing Deng},
      journal = {Neurocomputing},
      volume = {573},
      pages = {127225},
      year = {2024},
      issn = {0925-2312},
      doi = {https://doi.org/10.1016/j.neucom.2023.127225},
      url = {https://www.sciencedirect.com/science/article/pii/S0925231223013486},
  }
```
