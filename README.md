# Vulnerabilities and Defenses of Federated Learning
This is a list of papers reviewed in [A Survey on Vulnerability of Federated Learning: A Learning Algorithm Perspective](https://www.sciencedirect.com/science/article/pii/S0925231223013486).

![overview](overview.png)

## D2M
### Attacks
Poisoning Attacks against Support Vector Machines. [ACM](https://dl.acm.org/doi/10.5555/3042573.3042761)

Mitigating Sybils in Federated Learning Poisoning [arXiv](https://arxiv.org/abs/1808.04866)

Data Poisoning Attacks Against Federated Learning Systems [ACM](https://dl.acm.org/doi/10.1007/978-3-030-58951-6_24)

Semi-Targeted Model Poisoning Attack on Federated Learning via Backward Error Analysis [arXiv](https://arxiv.org/abs/2203.11633)

Attack of the Tails: Yes, You Really Can Backdoor Federated Learning [NIPS](https://proceedings.neurips.cc/paper/2020/hash/b8ffa41d4e492f0fad2f13e29e1762eb-Abstract.html)

PoisonGAN: Generative Poisoning Attacks Against Federated Learning in Edge Computing Systems [IEEE](https://ieeexplore.ieee.org/document/9194010)

Turning Federated Learning Systems Into Covert Channels [IEEE](https://ieeexplore.ieee.org/document/9984638)

Challenges and Approaches for Mitigating Byzantine Attacks in Federated Learning [arXiv](https://arxiv.org/abs/2112.14468)

Turning Privacy-preserving Mechanisms against Federated Learning [arXiv](https://arxiv.org/abs/2305.05355)

Local Environment Poisoning Attacks on Federated Reinforcement Learning [arXiv](https://arxiv.org/abs/2303.02725)

Data Poisoning Attacks on Federated Machine Learning [arXiv](https://arxiv.org/abs/2004.10020)

Understanding Distributed Poisoning Attack in Federated Learning [IEEE](https://ieeexplore.ieee.org/document/8975792)

### Defenses
Mitigating Sybils in Federated Learning Poisoning [arXiv](https://arxiv.org/abs/1808.04866)

Data Poisoning Attacks Against Federated Learning Systems [ACM](https://dl.acm.org/doi/10.1007/978-3-030-58951-6_24)

Understanding Distributed Poisoning Attack in Federated Learning [IEEE](https://ieeexplore.ieee.org/document/8975792)

Local Environment Poisoning Attacks on Federated Reinforcement Learning [arXiv](https://arxiv.org/abs/2303.02725)


## M2M
Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent https://papers.nips.cc/paper_files/paper/2017/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html

Generalized Byzantine-tolerant SGD https://arxiv.org/abs/1802.10116

A Little Is Enough: Circumventing Defenses For Distributed Learning https://proceedings.neurips.cc/paper/2019/hash/ec1c59141046cd1866bbbcdfb6ae31d4-Abstract.html

The Hidden Vulnerability of Distributed Learning in Byzantium http://proceedings.mlr.press/v80/mhamdi18a

Local model poisoning attacks to byzantine-robust federated learning https://dl.acm.org/doi/abs/10.5555/3489212.3489304

Free-rider Attacks on Model Aggregation in Federated Learning http://proceedings.mlr.press/v130/fraboni21a

Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent https://dl.acm.org/doi/10.1145/3154503

Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates https://proceedings.mlr.press/v80/yin18a.html

# Citation

```
  @article{XIE2024127225,
  title = {A survey on vulnerability of federated learning: A learning algorithm perspective},
  author = {Xianghua Xie and Chen Hu and Hanchi Ren and Jingjing Deng},
  journal = {Neurocomputing},
  volume = {573},
  pages = {127225},
  year = {2024},
  issn = {0925-2312},
  doi = {https://doi.org/10.1016/j.neucom.2023.127225},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231223013486},
  }
```
